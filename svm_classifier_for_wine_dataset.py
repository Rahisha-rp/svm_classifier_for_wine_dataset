# -*- coding: utf-8 -*-
"""SVM Classifier for Wine Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fvsJ1Q4T-Tg0hF_Xdwe3gG7Pa7FASaud

***Multi-Class Classification of Wine Dataset Using Linear SVM***
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import label_binarize
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc

# Load Dataset
from sklearn.datasets import load_wine
data = load_wine()

dataset = pd.DataFrame(data.data, columns=data.feature_names)
dataset.shape

dataset.head()

sample_features = data.data[0]
sample_target = data.target[0]
print("Feature Names:\n", data.feature_names)
print("\nThese are the sample features: ", sample_features)
print("\nTarget Class: ", sample_target)

X = data.data
y = data.target

n_classes = len(np.unique(y))
n_classes

# Split the dataset for 70% training and 30% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

# Linear SVM Classifier
svm = SVC(kernel = 'linear', decision_function_shape='ovr')

svm.fit(X_train, y_train)

# To make predictions on the test dataset
y_pred = svm.predict(X_test)

# Calculate accuracy score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of SVM classifier on wine dataset: ",accuracy)

# Evaluate confusion matrix
c_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(5,3))
sns.heatmap(c_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Binarize the output for ROC curve
y_test_binarized = label_binarize(y_test, classes = [0, 1, 2])
y_pred_binarized = label_binarize(y_pred, classes = [0, 1, 2])

fpr = dict()
tpr = dict()
roc_val = dict()
for i in range(3):
  fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_binarized[:, i])
  roc_val[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(6,5))
colors = ['gray', 'blue', 'red']
for i in range(3):
    plt.plot(fpr[i], tpr[i], color=colors[i], label=f'Class {i} (AUC = {roc_val[i]:.2f})')

plt.plot([0, 1], [0,1], 'k--', lw=2)
plt.title("Receiving Operating Characteristic (ROC) Curve")
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR or Recall)")
plt.legend(loc='lower right')
plt.show()

# Perform K-fold (k=5) cross validation to test the model's generalization
cross_val_scores = cross_val_score(svm, X_train, y_train, cv=5)

print("Accuracy for each fold: ", cross_val_scores)

print("Mean accuracy: ", cross_val_scores.mean())